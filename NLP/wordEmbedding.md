https://www.youtube.com/watch?v=viZrOnJclY0

(Another good tutorial: https://www.youtube.com/watch?v=lEzzgLh_SFA&list=WL&index=20)

# Summary: instead hardcoding one hot vector, we aim to learn the semantic vectors of each word in the semantic space. This semantic space can be generalized to other fields as well. In other words, the similar words, like 'good' and 'great', will have similar semantic vecotrs in the semantic space provided by the trained word embedding model.

<img width="1252" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/8bb9a859-3b39-434e-8320-4e788cfd7fbc">

<img width="695" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/5d701cca-1001-41fa-8658-68ad2624d794">

<img width="1076" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/a1bc865d-98b3-4cb6-8951-ffcbe3791406">

# namely, the number of basises of the semantic space

<img width="1634" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/1cf208a1-117e-43e6-a1bd-ae430119b729">

<img width="1209" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/b241f5b5-a90d-434c-9c90-cd1fa80b752f">

<img width="1577" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/24d2c70d-995a-41f4-bf37-fcb2c94dae68">

<img width="1168" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/a27b9a9d-d493-47d1-8437-fa07d19f51ce">

<img width="1194" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/4e9a706a-82ad-4311-b532-b4a09218d9e7">

<img width="555" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/bfbccb72-86e7-488b-8932-fb5cc04aaf0d">

<img width="1571" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/0e63e537-05ef-426c-b35d-d92bb4795c25">

<img width="1148" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/d5918464-5df5-46ec-8a94-e1f2fffde151">

## 2 strategies that word2vec, a popular method for creating word embeddings, uses to include more context

<img width="787" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/c618f23e-9853-4e39-925f-411c7bc7efc5">

<img width="1082" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/0919fbf5-6471-4849-a37e-b34a58326fbf">

<img width="1028" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/d6b11e2b-4bd2-41c5-b3b5-8629094cb330">

<img width="746" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/0a141ae0-6245-4af9-92c3-5343879046c0">

<img width="1533" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/4b3c40ef-8503-4e7f-b1ef-05e2101670de">

<img width="645" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/709ca5a5-226f-4b01-8495-57de97adf3f0">

<img width="1523" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/54d0b324-9e37-48f6-b8ea-9be7722f0479">

<img width="672" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/b654ac0e-e05a-4ec5-b794-d1630e795beb">

<img width="584" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/215887b4-3bb5-4cb9-853f-11788b11a4e9">

<img width="722" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/eb011530-5925-48fe-a701-0dd734df78ab">

<img width="1490" alt="image" src="https://github.com/Dingyi-Kang/Machine_learning/assets/81428296/560d056a-8c39-4d58-9537-9c65994f79fe">

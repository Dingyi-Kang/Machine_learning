Good tutorials: 

### What is cross entropy https://www.youtube.com/watch?v=6ArSys5qHAU
#### we choose softmax over argmax becuase softmax has good/simple derivative while argmax has comlicated ones
<img width="1115" alt="image" src="https://user-images.githubusercontent.com/81428296/225217531-f8ee0f4e-d5f2-41ab-b01d-d5ebe2697a1e.png">

#### reason: why choose cross-entropy over square residues when using softmax as output layer
<img width="1082" alt="image" src="https://user-images.githubusercontent.com/81428296/225201140-9cd75afd-e1a4-49aa-bf6c-41aa677f126a.png">


### how to do softMax derivatice (it is very easy to follow but not complete)
https://www.youtube.com/watch?v=M59JElEPgIg&t=1s


### good papers about "Derivative of the Softmax Function and the Categorical Cross-Entropy Loss"
https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1

<img width="738" alt="image" src="https://user-images.githubusercontent.com/81428296/225216471-3ed88e61-662f-4383-ac89-8e5cd7883b18.png">
<img width="750" alt="image" src="https://user-images.githubusercontent.com/81428296/225216555-ad00396f-e634-4144-942b-ba1dfc8a791f.png">
<img width="703" alt="image" src="https://user-images.githubusercontent.com/81428296/225217133-cf93ddd6-c838-44c0-980a-ac63f1d46d71.png">
<img width="738" alt="image" src="https://user-images.githubusercontent.com/81428296/225217191-a00c038c-aee4-4012-8943-a3fc91e3307f.png">

#### which is equivalent to 
<img width="290" alt="image" src="https://user-images.githubusercontent.com/81428296/225217248-5126da50-6506-40d8-8240-db5e76095d8a.png">




### another okay paper about it (it is not as clear as the above one. but still helpful in terms explaining derative of softmax)
https://deepnotes.io/softmax-crossentropy



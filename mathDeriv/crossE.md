Good tutorials: 

### What is cross entropy
#### we choose softmax over argmax becuase softmax has good/simple derivative while argmax has comlicated ones
#### reason: why choose cross-entropy over square residues when using softmax as output layer
https://www.youtube.com/watch?v=6ArSys5qHAU
<img width="1082" alt="image" src="https://user-images.githubusercontent.com/81428296/225201140-9cd75afd-e1a4-49aa-bf6c-41aa677f126a.png">


### how to do softMax derivatice (it is very easy to follow but not complete)
https://www.youtube.com/watch?v=M59JElEPgIg&t=1s


### good papers about "Derivative of the Softmax Function and the Categorical Cross-Entropy Loss"
https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1


### another okay paper about it (it is not as clear as the above one. but still helpful in terms explaining derative of softmax)
https://deepnotes.io/softmax-crossentropy
